---
output:
  pdf_document: default
  html_document: default
---
\newpage
# Introduction

India is facing a learning crisis. In 2018, nearly half of all rural students in grade five couldn’t read a grade two text and two thirds couldn’t perform simple division [@aser2018]. While opinions vary on how best to address the learning crisis, there is widespread agreement that data on learning outcomes will be key to finding solutions. The World Bank, in the 2018 World Development Report on education, urges countries “to take learning seriously, start by measuring it” [@filmer2018learning]. The central government think tank in India, the NITI Aayog recently launched an index of state education quality which relies, in large part, on data on learning outcomes to spur “competitive federalism” between states^[https://www.thehindu.com/opinion/op-ed/on-the-learning-curve/article26583825.ece]. Data on learning outcomes will be especially important as the centre and states take up recommendations from the recently published National Education Policy and for successful implementation of the recently announced National Foundational Literacy and Numeracy Mission.

In this paper, we take stock of India’s data on learning outcomes. In particular, we assess the accuracy and precision of data from India’s two nationally representative learning outcomes surveys: the Annual State of Education Report (ASER) basic survey, conducted by the independently run ASER Centre, and the National Achievement Survey (NAS), conducted by the central government with the help of the states. The ASER basic survey was conducted every year from 2005 to 2014 and every other year from 2014, is representative of all rural households, and seeks to measure whether children have attained basic foundational literacy and numeracy. ASER was the first (to our knowledge) nationally representative survey of learning outcomes and played a pivotal role in raising awareness of India’s low learning levels.

The NAS (in its current, expanded format) has only been conducted once, in 2017, but the central government plans to conduct it regularly. \textcolor{red}{Is there a citation available for this?} NAS is administered in school to children in grades 3, 5, and 8^[In a separate follow-up survey, NAS was also administered to 10th class students.] and seeks to measure whether students have achieved grade-level learning objectives. In addition to these two sources of data on learning outcomes based on sample surveys, other potential sources of data on learning outcomes include state summative assessments and results from the board exams, administered at the end of classes 10 and 12. We do not consider summative assessments as these vary widely by states and are not made available to the public. Similarly, we do not consider board exams as a substantial portion of students do not complete grade 10 and state boards vary widely by state.

We first compare NAS and ASER data to each other and to a third source of data on learning outcomes, the India Human Development Survey (IHDS) [@desai2015india]. ASER and IHDS use a virtually identical assessment tool and a similar sampling strategy. By contrast, NAS uses a different assessment tool and sampling strategy. To ensure comparability across datasets, we focus on students and schools which are included in all datasets (rural class 3 students in government schools) and learning outcomes which are most similar across the datasets (reading outcomes).

After restricting the dataset samples, we find that ASER and IHDS state averages are very similar to each other. While it is unsurprising given that the two datasets use the same tool and a similar sampling strategy, this nevertheless provides reassurance in the accuracy of ASER state averages. By contrast, we find that NAS state averages are significantly higher than both ASER and IHDS averages. In addition, state rankings based on NAS data display almost no correlation with state rankings based on ASER, IHDS, or net state domestic product per capita.\textcolor{red}{I know that this comes up later, but maybe one sentence about why net state domestic product should be correlated?}

We show that the size of these discrepancies is larger than can be reasonably explained by differences in the latent reading ability being tested. We further provide suggestive evidence that voluntary student absence on NAS exam data is unlikely to be a major source of these discrepancies. We conclude that NAS state averages are likely artificially high and contain little information about states’ relative performance.

We next assess the internal reliability of ASER data. The ASER reading and math assessments have been analysed through comparisons to other widely used tools like the early grade reading assessment (EGRA). These comparisons found ASER to be reliable and valid and the sample size for the ASER survey is large enough to ensure reasonable precision [@vagh2012validating]. Yet, there are two reasons to suspect that there may be significant non-sampling errors in ASER data. First, ASER is implemented through the assistance of partner organizations which in turn often use volunteer surveyors with relatively little experience. Second, to sample households within villages, ASER uses the “right-hand rule,” in which surveyors walk around the village selecting every Xth household rather than the more accurate (but costly) household listing method. These are not criticisms of the ASER survey – without these cost-saving measures the survey would likely be prohibitively expensive – but they also raise the risk of bias or reduced precision. All ASER enumerators undergo standardized training but even slight differences in survey administration by partner organization may lead to large increases in variance of district or state averages.

To assess the reliability of ASER data, we use two approaches developed by [@kane2002promise] for decomposing the variance of scores into persistent and transitory components. We then further decompose variance arising from the transitory component into variance arising from sampling and variance arising from other sources. While we cannot further distinguish between transitory non-sampling variance arising due to surveying (such as partner fixed effects) or other sources (such as a temporary increase in learning outcomes), we show that learning level differences between cohorts are unlikely to be a cause of transitory changes in scores and provide qualitative arguments for why true changes in learning outcomes are unlikely to be the source of transitory changes in scores.

We apply these methods to state-level ASER data on the proportion of rural class 3 children who can read a standard 2 level text and the proportion who can perform simple subtraction. We also use these methods in the context of district-level data on the proportion of class 3, 4, and 5 students who can read a standard 1 level text and the proportion who can perform simple subtraction. We find that a relatively small portion (5-9%) of the overall variance in state scores is due to transitory effects. By contrast, a substantial portion (between one third and one half) of the variance in changes in state scores and the variance in district scores are due to transitory effects. Variance in changes in district scores is nearly entirely (>75%) due to transitory effects. Across subjects, aggregation levels, and levels vs changes, sampling error appears to make up a small portion of variance. \textcolor{red}{We introduce the language of "levels and changes" more clearly later on. We might want to either stay away from that here or introduce what that means}

If transitory effects are due to noise, these findings imply that ASER is reliable for static comparisons of state performance but care should be taken when using ASER to compare districts or state progress from one round to the next. Taking changes in state average reading scores as an example, approximately 40% of the variance in the changes is due to transitory effects. This implies that if we attempt to identify the top 25% of states in terms of reading gains, a third of the states identified would not actually be in the top 25%.

Our paper makes several contributions to the literature. First, there is an established body of work on understanding the reliability of test scores and assessment tools to capture educational attainment overtime and comparatively across geographies [@chay2005central]. This literature has been primarily focused on assessments in the US. There has been considerably less attention on the reliability and consistency in developing countries like India. 

The rest of this paper proceeds as follows. Section \@ref(learningdata) provides a brief overview of NAS, ASER, and IHDS. Section \@ref(comparison) describes the overall approach to comparing these three sources of learning data. Section \@ref(results) presents the main results of this comparison. Section \@ref(aserinternal) presents the analysis on the internal reliability of ASER data. Section \@ref(conclusion) concludes. 
